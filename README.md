# PrAutoParte Scraper - Sistema Profesional de Scraping

Scraper profesional para extraer datos de productos desde [PrAutoParte](https://www.prautopartes.com.ar/) con soporte completo para deployment en producci√≥n con PM2, Docker e integraci√≥n con Odoo.

## Caracter√≠sticas Principales

### üî• Scraping Profesional
- ‚úÖ Scraping automatizado con Selenium y requests
- ‚úÖ Manejo robusto de errores y reintentos
- ‚úÖ Logging detallado con rotaci√≥n de archivos
- ‚úÖ Configuraci√≥n por variables de entorno
- ‚úÖ Gesti√≥n autom√°tica de ChromeDriver
- ‚úÖ Exportaci√≥n a CSV estructurado

### üöÄ Deployment Profesional
- ‚úÖ Soporte para PM2 (process manager)
- ‚úÖ Configuraci√≥n Docker completa
- ‚úÖ Integraci√≥n autom√°tica con Odoo
- ‚úÖ Scheduling autom√°tico cada 4 horas
- ‚úÖ Monitoreo y reinicio autom√°tico
- ‚úÖ Gesti√≥n de memoria y recursos

### üåê Integraci√≥n Empresarial
- ‚úÖ API XML-RPC para Odoo
- ‚úÖ Sincronizaci√≥n autom√°tica de productos
- ‚úÖ Gesti√≥n de categor√≠as por marca
- ‚úÖ Actualizaci√≥n de precios y stock
- ‚úÖ Manejo de productos duplicados

## Arquitectura del Sistema

```
prauto-scraper/
‚îú‚îÄ‚îÄ main.py                     # Script principal profesionalizado
‚îú‚îÄ‚îÄ requirements.txt            # Dependencias de Python
‚îú‚îÄ‚îÄ .env.example               # Ejemplo de variables de entorno
‚îú‚îÄ‚îÄ .env                      # Variables de entorno de producci√≥n
‚îú‚îÄ‚îÄ ecosystem.config.js       # Configuraci√≥n PM2
‚îú‚îÄ‚îÄ Dockerfile                # Imagen Docker
‚îú‚îÄ‚îÄ docker-compose.yml        # Orquestaci√≥n Docker
‚îú‚îÄ‚îÄ setup_linux.sh            # Script de instalaci√≥n autom√°tica
‚îú‚îÄ‚îÄ csv_manager.py            # Gesti√≥n de archivos CSV
‚îú‚îÄ‚îÄ logs/                     # Directorio de logs (autom√°tico)
‚îú‚îÄ‚îÄ output/                   # Directorio de salida (autom√°tico)
‚îú‚îÄ‚îÄ README.md                 # Documentaci√≥n completa
‚îî‚îÄ‚îÄ .gitignore               # Archivos a ignorar
```

## üöÄ Gu√≠a de Instalaci√≥n y Deployment

### Paso 1: Instalaci√≥n del Sistema

#### Opci√≥n A: Instalaci√≥n Automatizada (Recomendada)
```bash
# Clonar repositorio (si aplica)
git clone https://github.com/tustage3/stage3-performance-lube.git
cd stage3-performance-lube

# Ejecutar instalaci√≥n autom√°tica
chmod +x setup_linux.sh
./setup_linux.sh
```

#### Opci√≥n B: Instalaci√≥n Manual Complete

1. **Instalar dependencias del sistema:**

   **Ubuntu/Debian 20.04/22.04:**
   ```bash
   sudo apt update && sudo apt upgrade -y
   sudo apt install -y python3 python3-pip python3-venv python3-dev curl wget gnupg software-properties-common
   sudo apt install -y chromium-browser chromium-chromedriver xvfb
   ```

   **CentOS/RHEL 8/9:**
   ```bash
   sudo dnf update -y
   sudo dnf install -y python3 python3-pip python3-devel curl wget which
   sudo dnf install -y chromium chromedriver xorg-x11-server-Xvfb
   ```

   **Arch/Manjaro:**
   ```bash
   sudo pacman -Syu
   sudo pacman -S python python-pip python-virtualenv curl wget
   sudo pacman -S chromium chromedriver xorg-server-xvfb
   ```

2. **Instalar PM2 (Process Manager):**
   ```bash
   # Instalar Node.js y PM2
   curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
   sudo apt install -y nodejs
   sudo npm install -g pm2
   ```

3. **Configurar entorno Python:**
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

4. **Configurar variables de entorno:**
   ```bash
   cp .env.example .env
   nano .env  # Editar con tus credenciales
   ```

### Paso 2: Configuraci√≥n de Producci√≥n

#### Configuraci√≥n de Variables de Entorno
Crear y editar el archivo `.env`:

```bash
# Copiar template
cp .env.example .env
nano .env
```

Configurar las siguientes variables:

```env
# ===== CREDENCIALES PRAUTOPARTE =====
PRAUTO_USERNAME=tu_usuario_prautoparte
PRAUTO_PASSWORD=tu_contrase√±a_prautoparte

# ===== CONFIGURACI√ìN SCRAPER =====
HEADLESS=true  # Siempre true en producci√≥n
PYTHONPATH=/home/ubuntu/stage3-performance-lube
PYTHONUNBUFFERED=1

# ===== CONFIGURACI√ìN ODOO =====
ODOO_URL=http://your-odoo-server.com:8069
ODOO_DB=your_database_name
ODOO_USER=your_odoo_user
ODOO_PASSWORD=your_odoo_password
SEND_TO_ODOO=true  # true/false para enviar datos a Odoo

# ===== CONFIGURACI√ìN AVANZADA =====
PM2_LOG_DIR=/home/ubuntu/stage3-performance-lube/logs
OUTPUT_DIR=/home/ubuntu/stage3-performance-lube/output
```

#### Configuraci√≥n de Directorios
```bash
# Crear directorios necesarios
mkdir -p logs output
chmod 755 logs output
```

### Paso 3: Iniciar el Servicio

#### Opci√≥n A: Ejecuci√≥n √önica
```bash
# Activar entorno
source venv/bin/activate

# Ejecutar scraper una vez
python main.py --once
```

#### Opci√≥n B: PM2 Process Manager (Recomendado)
```bash
# Iniciar el proceso con PM2
pm2 start ecosystem.config.js

# Verificar estado
pm2 status
pm2 logs prauto-scraper

# Reiniciar si es necesario
pm2 restart prauto-scraper
```

#### Opci√≥n C: Docker
```bash
# Construir y ejecutar con Docker Compose
docker-compose up -d

# Ver logs
docker-compose logs -f prauto-scraper
```

### Paso 4: Configuraci√≥n de Monitoreo

#### Configuraci√≥n de Logs Rotativos
PM2 maneja autom√°ticamente la rotaci√≥n de logs. Para configurar:

```bash
# Instalar plugin de rotaci√≥n de logs
pm2 install pm2-logrotate

# Configurar rotaci√≥n diaria
pm2 set pm2-logrotate:max_size 10M
pm2 set pm2-logrotate:retain 30
pm2 set pm2-logrotate:compress true
```

#### Monitoreo del Sistema
```bash
# Ver procesos activos
pm2 monit

# Ver lista de procesos
pm2 list

# Ver uso de memoria y CPU
pm2 info prauto-scraper
```

## Instalaci√≥n Local (Windows)

1. **Configurar el entorno:**
   ```bash
   python -m venv venv
   venv\Scripts\activate
   pip install -r requirements.txt
   ```

2. **Configurar variables de entorno:**
   ```bash
   copy .env.example .env
   # Editar .env con tus credenciales
   ```

3. **Ejecutar el scraper:**
   ```bash
   python main.py
   ```

## Deployment con Docker

### Opci√≥n 1: Docker Build Manual
```bash
# Construir imagen
docker build -t prauto-scraper .

# Ejecutar contenedor
docker run --rm \
  --env-file .env \
  -v $(pwd)/output:/app/output \
  -v $(pwd)/logs:/app/logs \
  prauto-scraper
```

### Opci√≥n 2: Docker Compose
```bash
# Ejecutar una vez
docker-compose run --rm prauto-scraper

# O ejecutar en background
docker-compose up -d
```

### Opci√≥n 3: Con Scheduling (Cron)
```bash
# Crear archivo de cron
echo "0 2 * * * cd /app && python main.py" > crontab

# Ejecutar con cron
docker-compose --profile cron up -d scraper-cron
```

## ‚öôÔ∏è Configuraci√≥n y Variables de Entorno

### Variables de Entorno Esenciales

| Variable | Descripci√≥n | Valor Ejemplo | Obligatorio |
|----------|-------------|---------------|-------------|
| `PRAUTO_USERNAME` | Usuario para login PrAutoParte | `30-71727423-3` | ‚úÖ |
| `PRAUTO_PASSWORD` | Contrase√±a para login PrAutoParte | `10831` | ‚úÖ |
| `HEADLESS` | Ejecutar Chrome sin GUI | `true` | ‚ùå |
| `PYTHONPATH` | Ruta del proyecto | `/home/ubuntu/stage3-performance-lube` | ‚ùå |
| `PYTHONUNBUFFERED` | Buffer de Python | `1` | ‚ùå |

### Variables de Entorno para Odoo

| Variable | Descripci√≥n | Valor Ejemplo | Obligatorio |
|----------|-------------|---------------|-------------|
| `ODOO_URL` | URL del servidor Odoo | `http://localhost:8069` | ‚ùå |
| `ODOO_DB` | Nombre de la base de datos Odoo | `odoo` | ‚ùå |
| `ODOO_USER` | Usuario de Odoo | `admin` | ‚ùå |
| `ODOO_PASSWORD` | Contrase√±a de Odoo | `admin` | ‚ùå |
| `SEND_TO_ODOO` | Enviar datos a Odoo | `true/false` | ‚ùå |

### Variables de Entorno de Producci√≥n

| Variable | Descripci√≥n | Valor Ejemplo | Obligatorio |
|----------|-------------|---------------|-------------|
| `PM2_LOG_DIR` | Directorio de logs PM2 | `/home/ubuntu/stage3-performance-lube/logs` | ‚ùå |
| `OUTPUT_DIR` | Directorio de salida CSV | `/home/ubuntu/stage3-performance-lube/output` | ‚ùå |
| `NODE_ENV` | Entorno de Node.js | `production` | ‚ùå |

### Ejemplo de .env Completo
```env
# ===== CREDENCIALES PRAUTOPARTE =====
PRAUTO_USERNAME=tu_usuario_prautoparte
PRAUTO_PASSWORD=tu_contrase√±a_prautoparte

# ===== CONFIGURACI√ìN SCRAPER =====
HEADLESS=true
PYTHONPATH=/home/ubuntu/stage3-performance-lube
PYTHONUNBUFFERED=1

# ===== CONFIGURACI√ìN ODOO =====
ODOO_URL=http://your-odoo-server.com:8069
ODOO_DB=production_db
ODOO_USER=api_user
ODOO_PASSWORD=secure_password
SEND_TO_ODOO=true

# ===== CONFIGURACI√ìN AVANZADA =====
PM2_LOG_DIR=/home/ubuntu/stage3-performance-lube/logs
OUTPUT_DIR=/home/ubuntu/stage3-performance-lube/output
NODE_ENV=production
```

## Configuraci√≥n Avanzada

El scraper se puede configurar modificando la clase `ScrapingConfig` en `main.py`:

```python
@dataclass
class ScrapingConfig:
    base_url: str = "https://www.prautopartes.com.ar/"
    output_file: str = "articulos.csv"
    page_timeout: int = 10           # Timeout para cargar p√°ginas
    request_delay: float = 0.5       # Pausa entre peticiones API
    window_size: str = "1920,1080"   # Tama√±o de ventana del browser
```

## Logs

Los logs se guardan autom√°ticamente en el directorio `logs/` con:
- Rotaci√≥n diaria
- Retenci√≥n de 7 d√≠as
- Formato estructurado con timestamp

## Archivos de Salida

El scraper guarda los datos en archivos CSV con la fecha del scraping:

```
articulos_2025-09-20.csv  # Scraping del 20 de septiembre de 2025
articulos_2025-09-21.csv  # Scraping del 21 de septiembre de 2025
```

### Gesti√≥n de Archivos CSV

Usa el script `csv_manager.py` para gestionar los archivos:

```bash
# Listar archivos CSV disponibles
python csv_manager.py list

# Ver informaci√≥n detallada del √∫ltimo CSV
python csv_manager.py info

# Ver informaci√≥n de un CSV espec√≠fico
python csv_manager.py info 2025-09-20

# Comparar dos archivos CSV
python csv_manager.py compare 2025-09-20 2025-09-21

# Limpiar archivos antiguos (>7 d√≠as)
python csv_manager.py cleanup --days 7
```

| Campo | Descripci√≥n |
|-------|-------------|
| `id` | ID √∫nico del producto |
| `codigo` | C√≥digo del producto |
| `marca` | Marca del producto |
| `descripcion` | Descripci√≥n detallada |
| `precioLista` | Precio de lista |
| `precioCosto` | Precio de costo |
| `precioVenta` | Precio de venta |
| `descuentos` | Descuentos aplicables |
| `disponibilidad` | Estado de disponibilidad |
| `origen` | Origen del producto |
| `fotos` | URLs de fotos (separadas por coma) |

## üõ†Ô∏è Administraci√≥n y Mantenimiento

### Comandos PM2 Esenciales

```bash
# Iniciar y detener
pm2 start ecosystem.config.js
pm2 stop prauto-scraper
pm2 restart prauto-scraper
pm2 delete prauto-scraper

# Monitoreo
pm2 status
pm2 monit
pm2 logs prauto-scraper
pm2 info prauto-scraper

# Gesti√≥n de procesos
pm2 save              # Guardar procesos actuales
pm2 resurrect         # Restaurar procesos guardados
pm2 startup           # Configurar inicio autom√°tico
pm2 unstartup         # Desactivar inicio autom√°tico
```

### Actualizaci√≥n del Sistema

```bash
# Actualizar dependencias Python
source venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt --upgrade

# Actualizar Chrome y ChromeDriver
sudo apt update && sudo apt upgrade -y chromium-browser chromium-chromedriver

# Reiniciar servicio PM2
pm2 restart prauto-scraper
```

### Rotaci√≥n y Gesti√≥n de Logs

```bash
# Ver logs en tiempo real
pm2 logs prauto-scraper --lines 100

# Configurar rotaci√≥n autom√°tica
pm2 install pm2-logrotate
pm2 set pm2-logrotate:max_size 10M
pm2 set pm2-logrotate:retain 30
pm2 set pm2-logrotate:compress true

# Limpiar logs antiguos manualmente
find logs/ -name "*.log" -mtime +30 -delete
```

### üö® Troubleshooting Com√∫n

#### Errores de Autenticaci√≥n PrAutoParte
```bash
# Error: "Credenciales no encontradas"
- Verificar archivo .env existe
- Comprobar PRAUTO_USERNAME y PRAUTO_PASSWORD
- Probar credenciales manualmente en el sitio web

# Error: "Token de autorizaci√≥n no encontrado"
- Las credenciales pueden ser incorrectas
- El sitio web puede haber cambiado el login
- Verificar la estructura de sesi√≥n
```

#### Problemas con Chrome/ChromeDriver
```bash
# Error: "ChromeDriver not found"
- Usar el script setup_linux.sh
- Verificar instalaci√≥n: chromium-browser --version
- Revisar instalaci√≥n: chromedriver --version

# Error: "ChromeDriver cannot be killed"
- Matar procesos zombie: pkill -f chrome
- Limpiar procesos: pkill -f chromedriver
- Reiniciar servicio PM2

# Error de memoria Chrome
- Configurar l√≠mite de memoria en ecosystem.config.js
- Usar `--disable-dev-shm-usage` en Chrome options
- Aumentar memoria Docker: --memory="2g"
```

#### Errores de Conexi√≥n Odoo
```bash
# Error: "Fall√≥ la autenticaci√≥n con Odoo"
- Verificar URL, DB, usuario y contrase√±a
- Probar conexi√≥n manual: curl http://odoo-server:8069
- Verificar firewall y puertos

# Error: "Connection refused"
- Odoo no est√° corriendo
- Puerto 8069 bloqueado
- URL incorrecta en configuraci√≥n
```

#### Errores PM2
```bash
# PM2 no inicia autom√°ticamente
pm2 startup
pm2 save

# Proceso consume mucha memoria
pm2 restart prauto-scraper
# Ajustar max_memory_restart en ecosystem.config.js

# Logs no rotan
pm2 install pm2-logrotate
pm2 restart prauto-scraper
```

### üîí Consideraciones de Seguridad

#### Configuraci√≥n Segura
- Usar variables de entorno para credenciales
- No commitear archivo .env
- Usar HTTPS para Odoo si est√° disponible
- Configurar firewall para acceso a puertos

#### Permisos de Sistema
```bash
# Permisos recomendados
chmod 600 .env                    # Solo usuario due√±o
chmod 755 logs output             # Acceso para servidor web
chmod 700 venv                    # Solo usuario due√±o

# Crear usuario dedicado
sudo useradd -r -s /bin/false scraper
sudo chown -R scraper:scraper /home/ubuntu/stage3-performance-lube
```

#### Backup y Recuperaci√≥n
```bash
# Backup de configuraci√≥n
tar -czf backup_config.tar.gz .env ecosystem.config.js requirements.txt

# Backup de datos
tar -czf backup_data.tar.gz output/ logs/

# Recuperaci√≥n
tar -xzf backup_config.tar.gz
tar -xzf backup_data.tar.gz
pm2 restart prauto-scraper
```

### üìä Monitoreo y Alertas

#### M√©tricas Clave
- Tiempo de ejecuci√≥n promedio
- Cantidad de productos procesados
- Uso de memoria y CPU
- Errores de conexi√≥n
- Status de integraci√≥n Odoo

#### Alertas Sugeridas
```bash
# Monitorear uso de memoria
pm2 monit | grep prauto-scraper

# Verificar logs de error
grep -i error logs/scraper_$(date +%Y-%m-%d).log

# Verificar ejecuci√≥n reciente
ls -la output/articulos_$(date +%Y-%m-%d).csv
```

### üîß Configuraci√≥n Avanzada

### Ajustes de Rendimiento
El scraper se puede configurar modificando la clase `ScrapingConfig` en `main.py`:

```python
@dataclass
class ScrapingConfig:
    base_url: str = "https://www.prautopartes.com.ar/"
    catalog_url: str = "https://www.prautopartes.com.ar/catalogo"
    api_url: str = "https://www.prautopartes.com.ar/api/Articulos/Buscar"
    output_dir: str = "./output"

    # Configuraci√≥n Odoo
    odoo_url: str = "http://localhost:8069"
    odoo_db: str = "odoo"
    odoo_user: str = "admin"
    odoo_password: str = "admin"

    # Ajustes de rendimiento
    page_timeout: int = 10           # Timeout para cargar p√°ginas
    request_delay: float = 0.5       # Pausa entre peticiones API
    window_size: str = "1920,1080"   # Tama√±o de ventana del browser
    send_to_odoo: bool = True        # Enviar datos directamente a Odoo
    batch_size: int = 10             # Tama√±o de lote para Odoo
```

### Escalabilidad Horizontal
```bash
# Para m√∫ltiples instancias (modificar ecosystem.config.js)
instances: 'max',  # Usar todos los CPUs disponibles
exec_mode: 'cluster'  # Modo cluster

# Ejecutar m√∫ltiples scrapers
pm2 start ecosystem.config.js -i max
```

### Configuraci√≥n de Scheduling
```bash
# Modificar scheduling en ecosystem.config.js
cron_restart: '0 */4 * * *'  # Cada 4 horas

# O configurar via cron system
crontab -e
# Agregar: 0 */4 * * * cd /home/ubuntu/stage3-performance-lube && pm2 restart prauto-scraper
```

## üìù Licencia y T√©rminos de Uso

Este proyecto es para uso educativo y de desarrollo. **Es responsabilidad del usuario:**

- Respetar los t√©rminos de servicio de PrAutoParte
- No sobrecargar los servidores del sitio web objetivo
- Cumplir con las pol√≠ticas de robots.txt
- Mantener confidencialidad de credenciales y datos
- Usar el scraper de manera √©tica y responsable

**Aviso Legal:** El uso de este scraper es bajo su propio riesgo. Los desarrolladores no son responsables por el mal uso o consecuencias del uso de esta herramienta.